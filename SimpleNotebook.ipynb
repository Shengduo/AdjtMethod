{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shengduo/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "## Import standard librarys\n",
    "import torch\n",
    "import torchdiffeq\n",
    "import pickle\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import scipy.optimize as opt\n",
    "import numpy as np\n",
    "\n",
    "from torchdiffeq import odeint\n",
    "from xitorch.interpolate import Interp1D\n",
    "from scipy.interpolate import interp1d\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "## Import local classes and functions\n",
    "from MassFricParams import MassFricParams\n",
    "from TimeSequenceGen import TimeSequenceGen\n",
    "from AdjointMethod import AdjDerivs\n",
    "from GradientDescent import GradDescent, objGradFunc\n",
    "\n",
    "torch.set_default_dtype(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent on fixed $\\alpha = [k, m, g]$ and $V$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters\n",
    "alpha0 = torch.tensor([50., 1., 9.8])\n",
    "VT = torch.tensor([[1., 1.], [0., 5.]])\n",
    "\n",
    "# Alpha range\n",
    "alp_low = torch.tensor([50., 0.5, 1., 9.])\n",
    "alp_hi = torch.tensor([100., 2., 10., 10.])\n",
    "y0 = torch.tensor([0., 1.0, 1.0])\n",
    "\n",
    "# Start beta\n",
    "beta0 = torch.tensor([0.008, 0.012, 2.e0, 0.5])\n",
    "\n",
    "# Target beta\n",
    "beta_targ = torch.tensor([0.011, 0.016, 1.e0, 0.58])\n",
    "\n",
    "# Beta ranges\n",
    "beta_low = torch.tensor([0.001, 0.006, 0.5e-3, 0.3])\n",
    "beta_high = torch.tensor([0.021, 0.026, 5, 0.8])\n",
    "scaling = torch.tensor([1., 1., 1., 1.])\n",
    "\n",
    "# Other arguments for optAlpha function\n",
    "max_iters = 10\n",
    "maxFuncCalls = 200\n",
    "regularizedFlag = False\n",
    "noLocalSearch = True\n",
    "\n",
    "# Sequence specific parameters\n",
    "T = 5.\n",
    "NofTPts = 1000\n",
    "\n",
    "# Tolerance parameters\n",
    "this_rtol = 1.e-6\n",
    "this_atol = 1.e-8\n",
    "\n",
    "# Store the keywords for optAlpha\n",
    "kwgs = {\n",
    "    'y0' : y0, \n",
    "    'alpha0' : alpha0, \n",
    "    'VT' : VT,\n",
    "    'alp_low' : alp_low, \n",
    "    'alp_high' : alp_hi, \n",
    "    'max_iters' : max_iters, \n",
    "    'beta_this' : beta0, \n",
    "    'beta_targ' : beta_targ, \n",
    "    'beta_low' : beta_low, \n",
    "    'beta_high' : beta_high, \n",
    "    'regularizedFlag' : regularizedFlag, \n",
    "    'maxFuncCalls' : maxFuncCalls, \n",
    "    'noLocalSearch' : noLocalSearch, \n",
    "    'T' : T, \n",
    "    'NofTPts' : NofTPts, \n",
    "    'this_rtol': this_rtol, \n",
    "    'this_atol' : this_atol\n",
    "}\n",
    "\n",
    "# Function to get target v\n",
    "def generate_target_v(alpha, VT, beta, y0, this_rtol, this_atol, regularizedFlag):\n",
    "    # y0[1] = alpha[2]\n",
    "    targ_SpringSlider = MassFricParams(alpha, VT, beta, y0)\n",
    "    targ_SpringSlider.print_info()\n",
    "    targ_seq = TimeSequenceGen(T, NofTPts, targ_SpringSlider, \n",
    "                               rtol=this_rtol, atol=this_atol, regularizedFlag=regularizedFlag)\n",
    "    v = targ_seq.default_y[1, :], \n",
    "    t = targ_seq.t\n",
    "    return v[0], t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################  Total Iteration 0 ########################################\n",
      "--------------------  Mass and spring parameters  --------------------\n",
      "k:         tensor(50.)\n",
      "m:         tensor(1.)\n",
      "g:         tensor(9.8000)\n",
      "\n",
      "\n",
      "--------------------  Rate-and-state parameters  --------------------\n",
      "fr:        tensor(0.5800)\n",
      "a:         tensor(0.0110)\n",
      "b:         tensor(0.0160)\n",
      "DRS:       tensor(1.)\n",
      "y0:        tensor([0., 1., 1.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shengduo/InverseProblems/AdjtMethod/MassFricParams.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(s, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost in computing gradients:  0.435014009475708\n",
      "shit\n",
      "Time cost in computing gradients:  0.4206252098083496\n",
      "========================================\n",
      "Initial descent succeeds:  tensor(True)\n",
      "Observation:  tensor(0.0045)\n",
      "Gradient (scaled):  tensor([-2.0618e+00,  3.3595e+00,  1.4744e-03, -2.3494e-01])\n",
      "Relative error of observation:  tensor(0.0020)\n",
      "Time cost in computing gradients:  0.41469693183898926\n",
      "========================================\n",
      "The 1th descent succeeds:  tensor(True)\n",
      "Gradient (scaled):  tensor([-2.7948e-01,  2.7340e-01,  2.3961e-04, -2.1595e-02])\n",
      "Relative error of observation:  tensor(0.0006)\n",
      "Time cost in computing gradients:  0.4398937225341797\n",
      "========================================\n",
      "The 2th descent succeeds:  tensor(True)\n",
      "Gradient (scaled):  tensor([-0.1104, -0.0307,  0.0001, -0.0006])\n",
      "Relative error of observation:  tensor(0.0006)\n",
      "Time cost in computing gradients:  0.4125638008117676\n",
      "========================================\n",
      "The 3th descent succeeds:  tensor(True)\n",
      "Gradient (scaled):  tensor([-0.0939, -0.0498,  0.0001,  0.0008])\n",
      "Relative error of observation:  tensor(0.0006)\n",
      "Time cost in computing gradients:  0.4048426151275635\n",
      "========================================\n",
      "The 4th descent succeeds:  tensor(True)\n",
      "Gradient (scaled):  tensor([-0.0848, -0.0452,  0.0001,  0.0005])\n",
      "Relative error of observation:  tensor(0.0006)\n",
      "Time cost in computing gradients:  0.42890214920043945\n",
      "========================================\n",
      "The 5th descent succeeds:  tensor(True)\n",
      "Gradient (scaled):  tensor([-0.0006,  0.0012,  0.0002, -0.0027])\n",
      "Relative error of observation:  tensor(0.0006)\n",
      "Time cost in computing gradients:  0.4680483341217041\n",
      "========================================\n",
      "The 6th descent succeeds:  tensor(False)\n",
      "Gradient (scaled):  tensor([ 0.0126, -0.0237,  0.0001, -0.0009])\n",
      "Relative error of observation:  tensor(0.0006)\n",
      "Time cost in computing gradients:  0.5476272106170654\n",
      "========================================\n",
      "The 7th descent succeeds:  tensor(True)\n",
      "Gradient (scaled):  tensor([-1.0213e-05, -9.1651e-05,  1.5537e-04, -2.5842e-03])\n",
      "Relative error of observation:  tensor(0.0006)\n",
      "Time cost in computing gradients:  0.4373774528503418\n",
      "========================================\n",
      "The 8th descent succeeds:  tensor(True)\n",
      "Gradient (scaled):  tensor([ 2.9502e-05, -2.6540e-04,  1.5531e-04, -2.5722e-03])\n",
      "Relative error of observation:  tensor(0.0006)\n",
      "Time cost in computing gradients:  0.4291863441467285\n",
      "========================================\n",
      "The 9th descent succeeds:  tensor(True)\n",
      "Gradient (scaled):  tensor([ 2.7427e-06, -1.2819e-04,  1.5534e-04, -2.5813e-03])\n",
      "Relative error of observation:  tensor(0.0006)\n",
      "Time cost in computing gradients:  0.462007999420166\n",
      "========================================\n",
      "The 10th descent succeeds:  tensor(True)\n",
      "Gradient (scaled):  tensor([ 6.4060e-05, -3.8294e-04,  1.5522e-04, -2.5632e-03])\n",
      "Relative error of observation:  tensor(0.0006)\n",
      "The final predicted parameters:  tensor([0.0110, 0.0096, 2.0000, 0.5002])\n",
      "Optimal beta:  tensor([0.0110, 0.0096, 2.0000, 0.5002])\n"
     ]
    }
   ],
   "source": [
    "## Number of total alpha-beta iterations\n",
    "N_AllIters = 1\n",
    "this_alpha = alpha0\n",
    "this_beta = beta0\n",
    "\n",
    "## Run alpha-beta iterations\n",
    "for i in range(N_AllIters):\n",
    "    # Print out info\n",
    "    print(\"#\" * 40, \" Total Iteration {0} \".format(i) + \"#\" * 40)\n",
    "    \n",
    "    ## First optimize alpha\n",
    "    kwgs['alpha0'] = this_alpha\n",
    "    kwgs['beta_this'] = this_beta\n",
    "    \n",
    "    # Timing alpha\n",
    "    # Update this Alpha\n",
    "    # this_alpha = optAlpha(O_GAN, kwgs)\n",
    "    \n",
    "    \n",
    "    ## Run grad descent on beta\n",
    "    # Generate target v\n",
    "    v, t = generate_target_v(this_alpha, kwgs['VT'], kwgs['beta_targ'], kwgs['y0'], kwgs['this_rtol'], kwgs['this_atol'], kwgs['regularizedFlag'])\n",
    "    \n",
    "    # Run gradient descent\n",
    "    myGradBB = GradDescent(this_alpha, kwgs['alp_low'], kwgs['alp_high'], kwgs['VT'], \n",
    "                           this_beta, kwgs['beta_low'], kwgs['beta_high'], \n",
    "                           kwgs['y0'], v, t, \n",
    "                           objGrad_func = objGradFunc, scaling = scaling, \n",
    "                           max_steps = 10, stepping = 'BB', obs_rtol = 1e-5, lsrh_steps = 10, \n",
    "                           regularizedFlag = kwgs['regularizedFlag'], \n",
    "                           T = kwgs['T'], NofTPts = kwgs['NofTPts'], this_rtol = kwgs['this_rtol'], this_atol = kwgs['this_atol'])\n",
    "    \n",
    "    myGradBB.run()\n",
    "    \n",
    "    # Update parameters\n",
    "    this_beta = myGradBB.beta_optimal\n",
    "    print(\"Optimal beta: \", this_beta)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "908f1ff8de8ebdc69e26ff027962402cbc597b1f299fe84c63d7830c8ca59587"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
